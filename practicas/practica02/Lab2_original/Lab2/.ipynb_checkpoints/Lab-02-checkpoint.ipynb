{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of k-Nearest Neigbor Classifier\n",
    "\n",
    "\n",
    "class kNN():\n",
    "    def __init__(self, k = 3, exp = 2):\n",
    "    # constructor for kNN classifier \n",
    "    # k is the number of neighbor for local class estimation\n",
    "    # exp is the exponent for the Minkowski distance\n",
    "        self.k = k\n",
    "        self.exp = exp\n",
    "      \n",
    "    def fit(self, X_train, Y_train):\n",
    "    # training k-NN method\n",
    "    # X_train is the training data given with input attributes. n-th row correponds to n-th instance.\n",
    "    # Y_train is the output data (output vector): n-th element of Y_train is the output value for n-th instance in X_train.\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train   \n",
    "         \n",
    "    def getDiscreteClassification(self, X_test):\n",
    "    # predict-class k-NN method\n",
    "    # X_test is the test data given with input attributes. Rows correpond to instances\n",
    "    # Method outputs prediction vector Y_pred_test:  n-th element of Y_pred_test is the prediction for n-th instance in X_test\n",
    "    \n",
    "        Y_pred_test = [] #prediction vector Y_pred_test for all the test instances in X_test is initialized to empty list []\n",
    "\n",
    "   \n",
    "        for i in range(len(X_test)):   #iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i] #i-th test instance \n",
    "            \n",
    "            distances = []  #list of distances of the i-th test_instance for all the train_instance s in X_train, initially empty.\n",
    "          \n",
    "            for j in range(len(self.X_train)):  #iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j] #j-th training instance \n",
    "                distance = self.Minkowski_distance(test_instance, train_instance) #distance between i-th test instance and j-th training instance  \n",
    "                distances.append(distance) #add the distance to the list of distances of the i-th test_instance\n",
    "        \n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the correspondence with the classes of the training instances \n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index = self.Y_train.index)\n",
    "        \n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn =  df_nn[:self.k]\n",
    "            \n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training instances to \n",
    "            # the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the classes of those k-closed \n",
    "            # training instances. Method value_counts() computes the counts (number of occurencies) for each class in \n",
    "            # self.Y_train[df_knn.index] in dataframe predictions. \n",
    "            predictions = self.Y_train[df_knn.index].value_counts()\n",
    "                 \n",
    "            # the first element of the index predictions.index contains the class with the highest count; i.e. the prediction y_pred_test.\n",
    "            y_pred_test = predictions.index[0]\n",
    "\n",
    "            # add the prediction y_pred_test to the prediction vector Y_pred_test for all the test instances in X_test\n",
    "            Y_pred_test.append(y_pred_test)\n",
    "        \n",
    "        return Y_pred_test\n",
    "\n",
    "    \n",
    "    def Minkowski_distance(self, x1, x2):\n",
    "    # computes the Minkowski distance of x1 and x2 for two labeled instances (x1,y1) and (x2,y2)\n",
    "    \n",
    "        # Set initial distance to 0\n",
    "        distance = 0\n",
    "    \n",
    "        # Calculate Minkowski distance using the exponent exp\n",
    "        for i in range(len(x1)):\n",
    "            distance = distance + abs(x1[i] - x2[i])**self.exp\n",
    "        \n",
    "        distance = distance**(1/self.exp)\n",
    "    \n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('glass.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.34, random_state=10)\n",
    "\n",
    "\n",
    "# range for the values of parameter k for kNN\n",
    "\n",
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "\n",
    "trainAcc = np.zeros(len(k_range))\n",
    "testAcc = np.zeros(len(k_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for k  in  k_range:\n",
    "    clf = kNN(k)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.getDiscreteClassification(X_train)\n",
    "    Y_predTest = clf.getDiscreteClassification(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(k_range,trainAcc,'ro-',k_range,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.random import random\n",
    "\n",
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "\n",
    "data = pd.read_csv('glass.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.34, random_state=10)\n",
    "\n",
    "\n",
    "# range for the values of parameter exp for kNN\n",
    "\n",
    "exp_range = [2,  100, 10000]\n",
    "\n",
    "trainAcc = np.zeros(len(exp_range))\n",
    "testAcc = np.zeros(len(exp_range))\n",
    "\n",
    "\n",
    "index = 0 \n",
    "for exp  in  exp_range:\n",
    "    clf = kNN(k = 3, exp = exp)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.getDiscreteClassification(X_train)\n",
    "    Y_predTest = clf.getDiscreteClassification(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "   \n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(exp_range,trainAcc,'ro-',exp_range,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('exp')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('autoprice.csv')\n",
    "data.head()\n",
    "Y = data['class']\n",
    "X = data.drop(['class'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
